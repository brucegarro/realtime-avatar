services:
  # Fish Speech TTS Service (fast, multilingual)
  # Only started if TTS_BACKEND=fish_speech
  fish-speech:
    image: fishaudio/fish-speech:latest
    container_name: realtime-avatar-fish-speech
    command: ["--compile"]  # Enable torch.compile for faster inference (~0.3x RTF vs 3x without)
    ports:
      - "8002:7860"  # Gradio WebUI port
    volumes:
      # Mount checkpoints from host (required - downloaded via HuggingFace)
      - ~/fish-speech-checkpoints/openaudio-s1-mini:/app/checkpoints/openaudio-s1-mini:ro
      # Share assets for reference audio
      - ./runtime/assets:/app/assets:ro
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - avatar-network
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:7860/ | grep -q gradio || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # Fish Speech needs ~4 min for torch.compile warmup
    profiles:
      - fish_speech  # Only started when fish_speech profile is active

  # GPU Acceleration Service (TTS + Avatar generation)
  gpu-service:
    build:
      context: ./runtime
      # Use Ditto for audio-driven talking heads (default), or override with GPU_DOCKERFILE
      dockerfile: ${GPU_DOCKERFILE:-Dockerfile.ditto}
      args:
        CUDA: "${USE_CUDA:-false}"
    container_name: realtime-avatar-gpu
    ports:
      - "8001:8001"
    volumes:
      # Share assets with host and other services
      - ./runtime/assets:/app/assets:ro
      # Model cache for downloads
      - model-cache:/root/.cache
      # Shared output directory
      - gpu-output:/tmp/gpu-service-output
      # Ditto checkpoints (mounted from host)
      - ~/ditto-talkinghead:/app/ditto-checkpoints:ro
    environment:
      - PORT=8001
      - HOST=0.0.0.0
      - COQUI_TOS_AGREED=1  # Auto-accept TTS license
      - AVATAR_BACKEND=${AVATAR_BACKEND:-ditto}
      - TTS_BACKEND=${TTS_BACKEND:-fish_speech}  # fish_speech (fast, default) or xtts (stable)
      - FISH_SPEECH_URL=http://fish-speech:7860  # For fish_speech backend (Gradio API)
      - USE_CUDA=${USE_CUDA:-true}
    # Enable GPU access (for CUDA/NVIDIA)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - avatar-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Runtime Service (Orchestration + API)
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    container_name: realtime-avatar-runtime
    ports:
      - "8000:8000"
    volumes:
      # Mount source code for hot reload (dev mode) - writable for model downloads
      - ./runtime:/app/runtime
      # Share assets
      - ./runtime/assets:/app/assets:ro
      # Shared output with GPU service
      - gpu-output:/tmp/gpu-service-output:ro
      # Local output directory
      - ./runtime/outputs:/app/outputs
    environment:
      - MODE=local
      - DEVICE=cpu
      - LOG_LEVEL=info
      - USE_EXTERNAL_GPU_SERVICE=true
      - GPU_SERVICE_URL=http://gpu-service:8001
    depends_on:
      gpu-service:
        condition: service_healthy
    networks:
      - avatar-network
    security_opt:
      - seccomp:unconfined
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Evaluator (runs on-demand)
  evaluator:
    build:
      context: ./evaluator
      dockerfile: Dockerfile
    container_name: realtime-avatar-evaluator
    volumes:
      - ./evaluator:/app:rw
      - ./runtime/assets:/app/assets:ro
      - ./evaluator/outputs:/app/outputs:rw
    environment:
      - RUNTIME_URL=http://runtime:8000
      - MODE=local
    depends_on:
      - runtime
    networks:
      - avatar-network
    profiles:
      - evaluator  # Only runs when explicitly called

networks:
  avatar-network:
    driver: bridge

volumes:
  model-cache:
    driver: local
  gpu-output:
    driver: local
  fish-speech-cache:
    driver: local
